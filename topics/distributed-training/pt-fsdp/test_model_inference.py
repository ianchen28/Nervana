#!/usr/bin/env python3
"""
æ¨¡å‹æ¨ç†æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯è®­ç»ƒåæ¨¡å‹çš„æ•ˆæœï¼Œæ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œè®­ç»ƒåæ¨¡å‹çš„é¢„æµ‹ç»“æœ
"""

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
)
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from loguru import logger
import json
from datetime import datetime
from sklearn.metrics import classification_report, confusion_matrix
import random

# è®¾ç½®è¾“å‡ºç›®å½•
outputs_dir = "outputs"
logs_dir = "logs"
os.makedirs(outputs_dir, exist_ok=True)
os.makedirs(logs_dir, exist_ok=True)

# é…ç½®æ—¥å¿—
logger.add(
    os.path.join(logs_dir, "inference_test.log"),
    rotation="10 MB",
    retention="7 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
)

class ModelInferenceTester:
    """æ¨¡å‹æ¨ç†æµ‹è¯•ç±»"""
    
    def __init__(self, model_name="bert-base-uncased", device=None):
        self.model_name = model_name
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        logger.info(f"åˆå§‹åŒ–æ¨ç†æµ‹è¯•å™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_models(self, trained_model_path=None):
        """åŠ è½½åŸå§‹æ¨¡å‹å’Œè®­ç»ƒåçš„æ¨¡å‹"""
        logger.info("åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½æ•°æ®é›†ä»¥è·å–æ ‡ç­¾æ•°é‡
        dataset = load_dataset("ag_news")
        num_labels = len(set(dataset["train"]["label"]))
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        self.original_model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, num_labels=num_labels
        )
        self.original_model.to(self.device)
        self.original_model.eval()
        
        # åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
        if trained_model_path and os.path.exists(trained_model_path):
            logger.info(f"åŠ è½½è®­ç»ƒåçš„æ¨¡å‹: {trained_model_path}")
            checkpoint = torch.load(trained_model_path, map_location=self.device)
            self.trained_model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name, num_labels=num_labels
            )
            self.trained_model.load_state_dict(checkpoint['model_state_dict'])
            self.trained_model.to(self.device)
            self.trained_model.eval()
            
            # è·å–è®­ç»ƒå†å²
            self.training_history = checkpoint.get('training_history', {})
            self.final_eval_results = checkpoint.get('final_eval_results', {})
        else:
            logger.warning(f"è®­ç»ƒåçš„æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {trained_model_path}")
            self.trained_model = None
            self.training_history = {}
            self.final_eval_results = {}
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def prepare_test_data(self, sample_size=100):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        logger.info(f"å‡†å¤‡æµ‹è¯•æ•°æ®ï¼Œé‡‡æ ·æ•°é‡: {sample_size}")
        
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset("ag_news")
        
        # å¯¹æµ‹è¯•é›†è¿›è¡Œé‡‡æ ·
        test_data = dataset["test"]
        if sample_size < len(test_data):
            # éšæœºé‡‡æ ·
            indices = random.sample(range(len(test_data)), sample_size)
            sampled_data = test_data.select(indices)
        else:
            sampled_data = test_data
        
        # åˆ†è¯å¤„ç†
        def tokenize_fn(batch):
            return self.tokenizer(
                batch["text"], 
                truncation=True, 
                padding=False, 
                max_length=128
            )
        
        tokenized_data = sampled_data.map(tokenize_fn, batched=True, remove_columns=["text"])
        tokenized_data.set_format("torch")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            tokenized_data, 
            batch_size=16, 
            collate_fn=self.data_collator
        )
        
        return dataloader, sampled_data
    
    def predict_with_model(self, model, dataloader):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="é¢„æµ‹ä¸­"):
                batch = {k: v.to(self.device) for k, v in batch.items()}
                outputs = model(**batch)
                
                # è·å–é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)
                predictions = torch.argmax(logits, dim=-1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
                all_labels.extend(batch["labels"].cpu().numpy())
        
        return np.array(all_predictions), np.array(all_probabilities), np.array(all_labels)
    
    def compare_models(self, dataloader, test_data):
        """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
        logger.info("å¼€å§‹æ¨¡å‹æ¯”è¾ƒ...")
        
        # åŸå§‹æ¨¡å‹é¢„æµ‹
        logger.info("ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        orig_preds, orig_probs, labels = self.predict_with_model(self.original_model, dataloader)
        
        results = {
            "original_model": {
                "predictions": orig_preds,
                "probabilities": orig_probs,
                "labels": labels
            }
        }
        
        # è®­ç»ƒåæ¨¡å‹é¢„æµ‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.trained_model is not None:
            logger.info("ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
            trained_preds, trained_probs, _ = self.predict_with_model(self.trained_model, dataloader)
            results["trained_model"] = {
                "predictions": trained_preds,
                "probabilities": trained_probs,
                "labels": labels
            }
        
        # è®¡ç®—å‡†ç¡®ç‡
        orig_accuracy = np.mean(orig_preds == labels)
        results["original_model"]["accuracy"] = orig_accuracy
        
        if self.trained_model is not None:
            trained_accuracy = np.mean(trained_preds == labels)
            results["trained_model"]["accuracy"] = trained_accuracy
            improvement = trained_accuracy - orig_accuracy
            results["improvement"] = improvement
            
            logger.info(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {orig_accuracy:.4f}")
            logger.info(f"è®­ç»ƒåæ¨¡å‹å‡†ç¡®ç‡: {trained_accuracy:.4f}")
            logger.info(f"å‡†ç¡®ç‡æå‡: {improvement:.4f}")
        else:
            logger.info(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {orig_accuracy:.4f}")
        
        return results, test_data
    
    def create_comparison_report(self, results, test_data, sample_size=10):
        """åˆ›å»ºæ¯”è¾ƒæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š...")
        
        # é€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œè¯¦ç»†æ¯”è¾ƒ
        sample_indices = random.sample(range(len(test_data)), min(sample_size, len(test_data)))
        
        report_data = []
        for idx in sample_indices:
            text = test_data[idx]["text"]
            true_label = test_data[idx]["label"]
            
            # è·å–æ ‡ç­¾åç§°
            label_names = ["World", "Sports", "Business", "Sci/Tech"]
            true_label_name = label_names[true_label]
            
            row = {
                "index": idx,
                "text": text[:100] + "..." if len(text) > 100 else text,
                "true_label": true_label_name,
                "true_label_id": true_label
            }
            
            # åŸå§‹æ¨¡å‹ç»“æœ
            orig_pred = results["original_model"]["predictions"][idx]
            orig_prob = results["original_model"]["probabilities"][idx]
            row.update({
                "orig_pred": label_names[orig_pred],
                "orig_pred_id": orig_pred,
                "orig_confidence": orig_prob[orig_pred],
                "orig_correct": orig_pred == true_label
            })
            
            # è®­ç»ƒåæ¨¡å‹ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "trained_model" in results:
                trained_pred = results["trained_model"]["predictions"][idx]
                trained_prob = results["trained_model"]["probabilities"][idx]
                row.update({
                    "trained_pred": label_names[trained_pred],
                    "trained_pred_id": trained_pred,
                    "trained_confidence": trained_prob[trained_pred],
                    "trained_correct": trained_pred == true_label
                })
            
            report_data.append(row)
        
        return pd.DataFrame(report_data)
    
    def plot_confusion_matrices(self, results, outputs_dir):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        logger.info("ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        
        fig, axes = plt.subplots(1, 2 if "trained_model" in results else 1, figsize=(15, 6))
        if "trained_model" not in results:
            axes = [axes]
        
        label_names = ["World", "Sports", "Business", "Sci/Tech"]
        
        # åŸå§‹æ¨¡å‹æ··æ·†çŸ©é˜µ
        orig_cm = confusion_matrix(
            results["original_model"]["labels"], 
            results["original_model"]["predictions"]
        )
        sns.heatmap(orig_cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=label_names, yticklabels=label_names, ax=axes[0])
        axes[0].set_title(f'åŸå§‹æ¨¡å‹æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {results["original_model"]["accuracy"]:.4f}')
        axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
        
        # è®­ç»ƒåæ¨¡å‹æ··æ·†çŸ©é˜µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "trained_model" in results:
            trained_cm = confusion_matrix(
                results["trained_model"]["labels"], 
                results["trained_model"]["predictions"]
            )
            sns.heatmap(trained_cm, annot=True, fmt='d', cmap='Greens',
                       xticklabels=label_names, yticklabels=label_names, ax=axes[1])
            axes[1].set_title(f'è®­ç»ƒåæ¨¡å‹æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {results["trained_model"]["accuracy"]:.4f}')
            axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
            axes[1].set_ylabel('çœŸå®æ ‡ç­¾')
        
        plt.tight_layout()
        confusion_path = os.path.join(outputs_dir, "confusion_matrices.png")
        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {confusion_path}")
        return confusion_path
    
    def plot_confidence_distribution(self, results, outputs_dir):
        """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒå›¾"""
        logger.info("ç”Ÿæˆç½®ä¿¡åº¦åˆ†å¸ƒå›¾...")
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('æ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=16)
        
        # åŸå§‹æ¨¡å‹ç½®ä¿¡åº¦
        orig_confidences = np.max(results["original_model"]["probabilities"], axis=1)
        axes[0, 0].hist(orig_confidences, bins=20, alpha=0.7, color='blue', edgecolor='black')
        axes[0, 0].set_title('åŸå§‹æ¨¡å‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('ç½®ä¿¡åº¦')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].grid(True, alpha=0.3)
        
        # è®­ç»ƒåæ¨¡å‹ç½®ä¿¡åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "trained_model" in results:
            trained_confidences = np.max(results["trained_model"]["probabilities"], axis=1)
            axes[0, 1].hist(trained_confidences, bins=20, alpha=0.7, color='green', edgecolor='black')
            axes[0, 1].set_title('è®­ç»ƒåæ¨¡å‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
            axes[0, 1].set_xlabel('ç½®ä¿¡åº¦')
            axes[0, 1].set_ylabel('é¢‘æ¬¡')
            axes[0, 1].grid(True, alpha=0.3)
            
            # ç½®ä¿¡åº¦å¯¹æ¯”
            axes[1, 0].hist(orig_confidences, bins=20, alpha=0.5, label='åŸå§‹æ¨¡å‹', color='blue')
            axes[1, 0].hist(trained_confidences, bins=20, alpha=0.5, label='è®­ç»ƒåæ¨¡å‹', color='green')
            axes[1, 0].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”')
            axes[1, 0].set_xlabel('ç½®ä¿¡åº¦')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # ç½®ä¿¡åº¦æå‡
            confidence_improvement = trained_confidences - orig_confidences
            axes[1, 1].hist(confidence_improvement, bins=20, alpha=0.7, color='orange', edgecolor='black')
            axes[1, 1].set_title('ç½®ä¿¡åº¦æå‡åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('ç½®ä¿¡åº¦æå‡')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
            axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)
            axes[1, 1].grid(True, alpha=0.3)
        else:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒåæ¨¡å‹ï¼Œéšè—å¤šä½™çš„å­å›¾
            axes[0, 1].set_visible(False)
            axes[1, 0].set_visible(False)
            axes[1, 1].set_visible(False)
        
        plt.tight_layout()
        confidence_path = os.path.join(outputs_dir, "confidence_distribution.png")
        plt.savefig(confidence_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ç½®ä¿¡åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {confidence_path}")
        return confidence_path
    
    def save_detailed_results(self, results, report_df, outputs_dir):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        logger.info("ä¿å­˜è¯¦ç»†ç»“æœ...")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        results_data = {
            "timestamp": datetime.now().isoformat(),
            "model_comparison": {
                "original_model_accuracy": results["original_model"]["accuracy"]
            }
        }
        
        if "trained_model" in results:
            results_data["model_comparison"]["trained_model_accuracy"] = results["trained_model"]["accuracy"]
            results_data["model_comparison"]["improvement"] = results["improvement"]
        
        # ä¿å­˜è®­ç»ƒå†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.training_history:
            results_data["training_history"] = self.training_history
        
        # ä¿å­˜ç»“æœåˆ°JSON
        results_path = os.path.join(outputs_dir, "inference_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°CSV
        report_path = os.path.join(outputs_dir, "detailed_predictions.csv")
        report_df.to_csv(report_path, index=False, encoding='utf-8')
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        logger.info(f"é¢„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return results_path, report_path
    
    def run_full_test(self, trained_model_path=None, sample_size=100, report_sample_size=10):
        """è¿è¡Œå®Œæ•´çš„æ¨ç†æµ‹è¯•"""
        logger.info("å¼€å§‹å®Œæ•´çš„æ¨¡å‹æ¨ç†æµ‹è¯•...")
        
        # åŠ è½½æ¨¡å‹
        self.load_models(trained_model_path)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        dataloader, test_data = self.prepare_test_data(sample_size)
        
        # æ¯”è¾ƒæ¨¡å‹
        results, _ = self.compare_models(dataloader, test_data)
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        report_df = self.create_comparison_report(results, test_data, report_sample_size)
        
        # ç”Ÿæˆå¯è§†åŒ–
        confusion_path = self.plot_confusion_matrices(results, outputs_dir)
        confidence_path = self.plot_confidence_distribution(results, outputs_dir)
        
        # ä¿å­˜ç»“æœ
        results_path, report_path = self.save_detailed_results(results, report_df, outputs_dir)
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ¯ æ¨¡å‹æ¨ç†æµ‹è¯•å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {sample_size}")
        print(f"ğŸ“ˆ åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {results['original_model']['accuracy']:.4f}")
        
        if "trained_model" in results:
            print(f"ğŸš€ è®­ç»ƒåæ¨¡å‹å‡†ç¡®ç‡: {results['trained_model']['accuracy']:.4f}")
            print(f"ğŸ“ˆ å‡†ç¡®ç‡æå‡: {results['improvement']:.4f}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  ğŸ“Š æ··æ·†çŸ©é˜µ: {confusion_path}")
        print(f"  ğŸ“ˆ ç½®ä¿¡åº¦åˆ†å¸ƒ: {confidence_path}")
        print(f"  ğŸ“‹ è¯¦ç»†ç»“æœ: {results_path}")
        print(f"  ğŸ“„ é¢„æµ‹æŠ¥å‘Š: {report_path}")
        print(f"  ğŸ“ æµ‹è¯•æ—¥å¿—: {logs_dir}/inference_test.log")
        
        return results, report_df


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨æ¨¡å‹æ¨ç†æµ‹è¯•...")
    
    # æ£€æŸ¥è®­ç»ƒåçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
    trained_model_path = os.path.join(outputs_dir, "final_model.pt")
    if not os.path.exists(trained_model_path):
        logger.warning(f"è®­ç»ƒåçš„æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {trained_model_path}")
        logger.info("å°†åªæµ‹è¯•åŸå§‹æ¨¡å‹")
        trained_model_path = None
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelInferenceTester()
    
    # è¿è¡Œæµ‹è¯•
    results, report_df = tester.run_full_test(
        trained_model_path=trained_model_path,
        sample_size=200,  # æµ‹è¯•æ ·æœ¬æ•°é‡
        report_sample_size=20  # è¯¦ç»†æŠ¥å‘Šæ ·æœ¬æ•°é‡
    )
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
    print("\n" + "="*60)
    print("ğŸ” é¢„æµ‹ç¤ºä¾‹:")
    print("="*60)
    
    for i in range(min(5, len(report_df))):
        row = report_df.iloc[i]
        print(f"\nç¤ºä¾‹ {i+1}:")
        print(f"æ–‡æœ¬: {row['text']}")
        print(f"çœŸå®æ ‡ç­¾: {row['true_label']}")
        print(f"åŸå§‹æ¨¡å‹é¢„æµ‹: {row['orig_pred']} (ç½®ä¿¡åº¦: {row['orig_confidence']:.3f})")
        if 'trained_pred' in row:
            print(f"è®­ç»ƒåæ¨¡å‹é¢„æµ‹: {row['trained_pred']} (ç½®ä¿¡åº¦: {row['trained_confidence']:.3f})")
        print(f"åŸå§‹æ¨¡å‹æ­£ç¡®: {'âœ…' if row['orig_correct'] else 'âŒ'}")
        if 'trained_correct' in row:
            print(f"è®­ç»ƒåæ¨¡å‹æ­£ç¡®: {'âœ…' if row['trained_correct'] else 'âŒ'}")


if __name__ == "__main__":
    main()
