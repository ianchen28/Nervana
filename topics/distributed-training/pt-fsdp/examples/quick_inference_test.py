#!/usr/bin/env python3
"""
å¿«é€Ÿæ¨ç†æµ‹è¯•è„šæœ¬ - ç®€åŒ–ç‰ˆæœ¬
ç”¨äºå¿«é€ŸéªŒè¯è®­ç»ƒåæ¨¡å‹çš„æ•ˆæœ
"""

import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
)
from torch.utils.data import DataLoader
import random
from loguru import logger


def quick_model_test(trained_model_path=None, sample_size=50):
    """å¿«é€Ÿæ¨¡å‹æµ‹è¯•"""

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ•°æ®å’Œtokenizer
    logger.info("åŠ è½½æ•°æ®...")
    dataset = load_dataset("ag_news")
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased",
                                              use_fast=True)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = dataset["test"]
    sample_indices = random.sample(range(len(test_data)),
                                   min(sample_size, len(test_data)))
    sampled_data = test_data.select(sample_indices)

    def tokenize_fn(batch):
        return tokenizer(batch["text"],
                         truncation=True,
                         padding=False,
                         max_length=128)

    tokenized_data = sampled_data.map(tokenize_fn,
                                      batched=True,
                                      remove_columns=["text"])
    tokenized_data.set_format("torch")

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    dataloader = DataLoader(tokenized_data,
                            batch_size=16,
                            collate_fn=data_collator)

    # åŠ è½½æ¨¡å‹
    num_labels = len(set(dataset["train"]["label"]))
    label_names = ["World", "Sports", "Business", "Sci/Tech"]

    # åŸå§‹æ¨¡å‹
    logger.info("åŠ è½½åŸå§‹æ¨¡å‹...")
    original_model = AutoModelForSequenceClassification.from_pretrained(
        "bert-base-uncased", num_labels=num_labels)
    original_model.to(device)
    original_model.eval()

    # è®­ç»ƒåæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    trained_model = None
    if trained_model_path and os.path.exists(trained_model_path):
        logger.info("åŠ è½½è®­ç»ƒåçš„æ¨¡å‹...")
        checkpoint = torch.load(trained_model_path, map_location=device)
        trained_model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased", num_labels=num_labels)
        trained_model.load_state_dict(checkpoint['model_state_dict'])
        trained_model.to(device)
        trained_model.eval()

    def predict_with_model(model, dataloader):
        """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        predictions = []
        labels = []

        with torch.no_grad():
            for batch in dataloader:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch)
                preds = torch.argmax(outputs.logits, dim=-1)
                predictions.extend(preds.cpu().numpy())
                labels.extend(batch["labels"].cpu().numpy())

        return np.array(predictions), np.array(labels)

    # è¿›è¡Œé¢„æµ‹
    logger.info("ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    orig_preds, true_labels = predict_with_model(original_model, dataloader)
    orig_accuracy = np.mean(orig_preds == true_labels)

    print(f"\n{'='*50}")
    print(f"ğŸ¯ å¿«é€Ÿæ¨¡å‹æµ‹è¯•ç»“æœ")
    print(f"{'='*50}")
    print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {sample_size}")
    print(f"ğŸ“ˆ åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {orig_accuracy:.4f}")

    if trained_model is not None:
        logger.info("ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        trained_preds, _ = predict_with_model(trained_model, dataloader)
        trained_accuracy = np.mean(trained_preds == true_labels)
        improvement = trained_accuracy - orig_accuracy

        print(f"ğŸš€ è®­ç»ƒåæ¨¡å‹å‡†ç¡®ç‡: {trained_accuracy:.4f}")
        print(f"ğŸ“ˆ å‡†ç¡®ç‡æå‡: {improvement:.4f}")

        # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
        print(f"\nğŸ” é¢„æµ‹ç¤ºä¾‹ (å‰5ä¸ªæ ·æœ¬):")
        print(f"{'='*50}")

        for i in range(min(5, len(sampled_data))):
            text = sampled_data[i]["text"]
            true_label = true_labels[i]
            orig_pred = orig_preds[i]
            trained_pred = trained_preds[i]

            print(f"\nç¤ºä¾‹ {i+1}:")
            print(f"æ–‡æœ¬: {text[:80]}...")
            print(f"çœŸå®æ ‡ç­¾: {label_names[true_label]}")
            print(
                f"åŸå§‹æ¨¡å‹é¢„æµ‹: {label_names[orig_pred]} {'âœ…' if orig_pred == true_label else 'âŒ'}"
            )
            print(
                f"è®­ç»ƒåæ¨¡å‹é¢„æµ‹: {label_names[trained_pred]} {'âœ…' if trained_pred == true_label else 'âŒ'}"
            )

            if orig_pred != trained_pred:
                print(
                    f"ğŸ”„ é¢„æµ‹å‘ç”Ÿå˜åŒ–: {label_names[orig_pred]} â†’ {label_names[trained_pred]}"
                )
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°è®­ç»ƒåçš„æ¨¡å‹æ–‡ä»¶: {trained_model_path}")
        print("åªæµ‹è¯•äº†åŸå§‹æ¨¡å‹")

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥è®­ç»ƒåçš„æ¨¡å‹
    trained_model_path = "outputs/final_model.pt"

    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    quick_model_test(
        trained_model_path=trained_model_path,
        sample_size=100  # å¯ä»¥è°ƒæ•´æ ·æœ¬æ•°é‡
    )


if __name__ == "__main__":
    main()
